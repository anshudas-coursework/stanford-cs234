{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Gymnasium Environment for the LambertPulser\n",
    "\n",
    "## Description\n",
    "The LambertPulser is a Gymnasium environment for a pulsable rocket in space to reach a specified point in a specified amount of time in space under the influence of gravity from another body. The rocket is allowed to make orbital transfers at any point in.\n",
    "\n",
    "The problem is formulated as a finite-thrust minimum fuel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Here, we import the relevant packages for creating the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment-related packages\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "from gym.wrappers import FlattenObservation\n",
    "\n",
    "# Neural network formulation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Mathematics packages\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.integrate import solve_ivp\n",
    "import random\n",
    "\n",
    "# Support functions\n",
    "import argparse, pdb\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "The configuration contains all hyperparameters and parameters required for defining the environment, the learnable agent, and storing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Bounds:\n",
    "    # State bounds\n",
    "    r: list[float]      # Radial distance from orbital center\n",
    "    theta: list[float]  # True anomaly\n",
    "    v_r: list[float]    # Radial velocity\n",
    "    v_t: list[float]    # Tangential velocity\n",
    "    rf: list[float]     # Remaining amount of fuel, lower bound = 0\n",
    "    rt: list[float]     # Remaining amount of time-to-go\n",
    "\n",
    "    # Action bounds\n",
    "    T: list[float]      # Thrust\n",
    "    phi: list[float]    # Direction of thrust\n",
    "    pT: int             # Pulse the thrust or not (0 or 1)\n",
    "\n",
    "@dataclass \n",
    "class Seeds:\n",
    "    # State randomization\n",
    "    r: int\n",
    "    theta: int\n",
    "    v_r: int\n",
    "    v_t: int\n",
    "    rf: int\n",
    "    rt: int\n",
    "    \n",
    "    # Action randomization\n",
    "    T: int\n",
    "    phi: int\n",
    "    pT: int\n",
    "\n",
    "@dataclass\n",
    "class Target:\n",
    "    r: float\n",
    "    theta: float\n",
    "    v_r: float\n",
    "    v_t: float\n",
    "\n",
    "@dataclass\n",
    "class Dynamics:\n",
    "    mu: float = 100\n",
    "    c: float = 5\n",
    "    update: float = 0.1\n",
    "\n",
    "class LPConfig:\n",
    "    def __init__(self, seed=1):\n",
    "        self.env_name = \"LambertPulser-v1\"\n",
    "        self.record = False\n",
    "\n",
    "        # Randomization\n",
    "        self.seed = Seeds(1,1,1,1,1,1,2,2,2)\n",
    "        seed_str = \"seed=\" + str(self.seed)\n",
    "\n",
    "        # State-action bounds\n",
    "        self.bounds = Bounds(\n",
    "            [0, 10**3],     # r\n",
    "            [0 ,2*np.pi],   # theta\n",
    "            [-10, 10],      # v_r\n",
    "            [-100, 100],    # v_t\n",
    "            [0, 100.0],     # rf\n",
    "            [0, 20.0],      # rt\n",
    "            [10**2, 10**3],        # T\n",
    "            [0, 2*np.pi],   # phi\n",
    "            2               # pT, on/off\n",
    "        )\n",
    "\n",
    "        # Dynamics\n",
    "        self.dynamics = Dynamics(\n",
    "            100,\n",
    "            5\n",
    "        )\n",
    "\n",
    "        # output config\n",
    "        self.output_path = \"results/{}-{}/\".format( self.env_name, seed_str )\n",
    "        self.model_output = self.output_path + \"model.weights/\"\n",
    "        self.log_path = self.output_path + \"log.txt\"\n",
    "        self.scores_output = self.output_path + \"scores.npy\"\n",
    "        self.plot_output = self.output_path + \"scores.png\"\n",
    "        self.record_path = self.output_path\n",
    "        self.record_freq = 5\n",
    "        self.summary_freq = 1\n",
    "\n",
    "        # model and training config\n",
    "        self.num_batches = 100  # number of batches trained on\n",
    "        self.batch_size = 2000  # number of steps used to compute each policy update\n",
    "        self.max_ep_len = 200  # maximum episode length\n",
    "        self.learning_rate = 3e-2\n",
    "        self.gamma = 1.0  # the discount factor\n",
    "        self.normalize_advantage = True\n",
    "\n",
    "        # parameters for the policy and baseline models\n",
    "        self.n_layers = 1\n",
    "        self.layer_size = 64\n",
    "\n",
    "        # hyperparameters for PPO\n",
    "        self.eps_clip = 0.2\n",
    "        self.update_freq = 5\n",
    "\n",
    "        # since we start new episodes for each batch\n",
    "        assert self.max_ep_len <= self.batch_size\n",
    "        if self.max_ep_len < 0:\n",
    "            self.max_ep_len = self.batch_size\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambertPulser(Env):\n",
    "    metadata = {\n",
    "        \"render_modes\":[\"human\", \"anim\"],\n",
    "        \"render_fps\": 24\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode=None, seed=1):\n",
    "        # Configuration\n",
    "        self.config = LPConfig(seed)\n",
    "\n",
    "        # Define the observation and action space\n",
    "        self.observation_space = Dict(\n",
    "            {\n",
    "                \"r\":        Box(self.config.bounds.r[0], self.config.bounds.r[1], dtype=float, shape=(1,)), \n",
    "                \"theta\":    Box(self.config.bounds.theta[0], self.config.bounds.theta[1], dtype=float, shape=(1,)), \n",
    "                \"v_r\":      Box(self.config.bounds.v_r[0], self.config.bounds.v_r[1], dtype=float, shape=(1,)),\n",
    "                \"v_t\":      Box(self.config.bounds.v_t[0], self.config.bounds.v_t[1], dtype=float, shape=(1,)),\n",
    "                \"dr\":       Box(0.0, self.config.bounds.r[1]-self.config.bounds.r[0], dtype=float, shape=(1,)), \n",
    "                \"dtheta\":   Box(0.0, self.config.bounds.theta[1]-self.config.bounds.theta[1], dtype=float, shape=(1,)), \n",
    "                \"dv_r\":     Box(0.0, self.config.bounds.v_r[1]-self.config.bounds.v_r[0], dtype=float, shape=(1,)),\n",
    "                \"dv_t\":     Box(0.0, self.config.bounds.v_t[1]-self.config.bounds.v_t[0], dtype=float, shape=(1,)),\n",
    "                \"rf\":       Box(self.config.bounds.rf[0], self.config.bounds.rf[1], dtype=float, shape=(1,)),\n",
    "                \"rt\":       Box(self.config.bounds.rt[0], self.config.bounds.rt[1], dtype=float, shape=(1,))\n",
    "            }\n",
    "        )\n",
    "        self.action_space = Dict(\n",
    "            {\n",
    "                \"T\":        Box(self.config.bounds.T[0], self.config.bounds.T[1], dtype=float, shape=(1,)), \n",
    "                \"phi\":      Box(self.config.bounds.phi[0], self.config.bounds.phi[1], dtype=float, shape=(1,)),\n",
    "                \"pulse\":    Discrete(self.config.bounds.pT)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.observation = self.reset()\n",
    "\n",
    "        # Target position\n",
    "        self.target = Target(0,0,0,0)\n",
    "\n",
    "        # Rendering options\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "    def reset(self):\n",
    "        # Radius bounds for sampling\n",
    "        r0_lower = self.config.bounds.r[0]\n",
    "        r0_upper = self.config.bounds.r[1]\n",
    "        r0_rand_bnd = [1/3, 2/3]\n",
    "\n",
    "        # Set the initial state to a circular orbit\n",
    "        r0 = r0_lower + (r0_upper - r0_lower) * np.random.uniform(r0_rand_bnd[0], r0_rand_bnd[1])\n",
    "        theta0 = np.random.uniform(self.config.bounds.theta[0], self.config.bounds.theta[1])\n",
    "        v_r0 = 0.0\n",
    "        v_t0 = np.sqrt(self.config.dynamics.mu / r0)\n",
    "        rf0 = np.random.uniform(self.config.bounds.rf[0], self.config.bounds.rf[1])\n",
    "\n",
    "        # Target\n",
    "        self.target = Target(\n",
    "            np.random.uniform(r0_lower, r0_upper),\n",
    "            np.random.uniform(self.config.bounds.theta[0], self.config.bounds.theta[1]),\n",
    "            np.random.uniform(self.config.bounds.v_r[0],self.config.bounds.v_r[1]),\n",
    "            np.random.uniform(self.config.bounds.v_t[0],self.config.bounds.v_t[1])\n",
    "        )\n",
    "\n",
    "        # Observation based on state and target\n",
    "        observation = {\n",
    "            \"r\":        r0,\n",
    "            \"theta\":    theta0,\n",
    "            \"v_r\":      v_r0,\n",
    "            \"v_t\":      v_t0,\n",
    "            \"dr\":       r0 - self.target.r,\n",
    "            \"dtheta\":   theta0 - self.target.theta,\n",
    "            \"dv_r\":     v_r0 - self.target.v_r,\n",
    "            \"dv_t\":     v_t0 - self.target.v_t,\n",
    "            \"rf\":       rf0,\n",
    "            \"rt\":       self.config.bounds.rt[1]-self.config.bounds.rt[0]\n",
    "        }\n",
    "\n",
    "        return observation\n",
    "    \n",
    "    def reward(self, action=None):\n",
    "        # Initialize\n",
    "        reward = 0\n",
    "\n",
    "        # Reach the desired point\n",
    "        reward += 0.25 * sp.stats.norm(0, 0.10*(self.config.bounds.r[1]-self.config.bounds.r[0])).pdf(self.observation[\"dr\"])\n",
    "        reward += 0.25 * sp.stats.norm(0, 0.10*(self.config.bounds.theta[1]-self.config.bounds.theta[0])).pdf(self.observation[\"dtheta\"])\n",
    "        reward += 0.05 * sp.stats.norm(0, 0.10*(self.config.bounds.v_r[1]-self.config.bounds.v_r[0])).pdf(self.observation[\"dv_r\"])\n",
    "        reward += 0.025 * sp.stats.norm(0, 0.10*(self.config.bounds.v_t[1]-self.config.bounds.v_t[0])).pdf(self.observation[\"dv_t\"])\n",
    "\n",
    "        # Use as little resources as possible\n",
    "        reward += (self.observation[\"rf\"])*0.025\n",
    "\n",
    "        # Complete the mission as quickly as possible\n",
    "        reward += (self.observation[\"rt\"])*0.01\n",
    "\n",
    "        # Encourage the thrusts to be fired in the direction of the motion\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def dynamics(self, action):\n",
    "        def ODE(t, y):\n",
    "            # Observations\n",
    "            obs = self.dict2array(self.observation)\n",
    "            y = obs[0,1,2,3,8]\n",
    "\n",
    "            # Actions\n",
    "            a = self.dict2array(action)\n",
    "\n",
    "            # State dynamics\n",
    "            dydt    = np.zeros(self.dict2array(self.observation).shape)\n",
    "            dydt[0] = y[2]\n",
    "            dydt[1] = y[3] / y[0]\n",
    "            dydt[2] = y[3]**2 / y[0] - self.config.dynamics.mu / y[0]**2 + a[2]*(a[0]*np.sin(a[1])) / y[4]\n",
    "            dydt[3] = y[2]*y[3] / y[0] + a[2]*(a[0]*np.cos(a[1])) / y[4]\n",
    "            dydt[4] = -a[2]*a[0] / self.config.dynamics.c\n",
    "            return dydt\n",
    "\n",
    "        res = solve_ivp(ODE, (0,self.config.dynamics.update), self.dict2array(self.observation), 'RK45', rtol = 1e-6)\n",
    "        \n",
    "\n",
    "    def step(self, action):\n",
    "        # Calculate the reward at the current step\n",
    "        reward = self.reward(action)\n",
    "\n",
    "        # Forward propagate the dynamics\n",
    "        self.dynamics(action)\n",
    "        observation = self.observation\n",
    "\n",
    "        # Determine termination criteria\n",
    "        done = False\n",
    "        #if ($$$NEED TO DO THIS$$$): done = True\n",
    "\n",
    "        # Dynamics model\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def dict2array(self, data):\n",
    "        return np.fromiter(data.values(), dtype=float)\n",
    "    \n",
    "    def array2dict(self, dict, data):\n",
    "        i = 0\n",
    "        for key in dict.keys():\n",
    "            dict[key] = data[i]\n",
    "            i += 1\n",
    "        return \n",
    "\n",
    "    def render(self, mode='none'):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.676835334371524"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create environment\n",
    "env = LambertPulser(render_mode=None)\n",
    "env.reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: The solver successfully reached the end of the integration interval.\n",
      "  success: True\n",
      "   status: 0\n",
      "        t: [ 0.000e+00  1.322e-05 ...  1.000e-01  1.000e-01]\n",
      "        y: [[ 4.820e+02  4.820e+02 ...  4.820e+02  4.820e+02]\n",
      "            [ 4.918e+00  4.918e+00 ...  4.918e+00  4.918e+00]\n",
      "            ...\n",
      "            [ 9.905e+01  9.905e+01 ...  9.905e+01  9.905e+01]\n",
      "            [ 2.000e+01  2.000e+01 ...  2.000e+01  2.000e+01]]\n",
      "      sol: None\n",
      " t_events: None\n",
      " y_events: None\n",
      "     nfev: 132140\n",
      "     njev: 0\n",
      "      nlu: 0\n"
     ]
    }
   ],
   "source": [
    "def ODE(t, y):\n",
    "    # Observations\n",
    "    obs = env.dict2array(env.observation)\n",
    "    y = obs[(0,1,2,3,8),]\n",
    "\n",
    "    # Actions\n",
    "    a = env.dict2array(env.action_space.sample())\n",
    "    a[2] = 1\n",
    "\n",
    "    # State dynamics\n",
    "    dydt    = np.zeros(env.dict2array(env.observation).shape)\n",
    "    dydt[0] = y[2]\n",
    "    dydt[1] = y[3] / y[0]\n",
    "    dydt[2] = y[3]**2 / y[0] - env.config.dynamics.mu / y[0]**2 + a[2]*(a[0]*np.sin(a[1])) / y[4]\n",
    "    dydt[3] = y[2]*y[3] / y[0] + a[2]*(a[0]*np.cos(a[1])) / y[4]\n",
    "    dydt[4] = -a[2]*a[0] / env.config.dynamics.c\n",
    "    return dydt\n",
    "\n",
    "# print(env.dict2array(env.observation))\n",
    "res = solve_ivp(ODE, (0,env.config.dynamics.update), env.dict2array(env.observation), 'LSODA', rtol = 1e-6)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.93367396913726"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 4\n",
    "res.y[idx,0]-res.y[idx,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs234-a3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "313195171dd84a7adbbc922761bdb2adf8b9ab2751e42588cf6a4d17d1cee90b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
